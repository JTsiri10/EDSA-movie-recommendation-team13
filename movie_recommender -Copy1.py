#!/usr/bin/env python
# coding: utf-8

# In[2]:


cd C:\Users\sjtsi\Documents\unsupervised\train.csv


# # Project : Movie Recommender system

# In[ ]:





# ### Team members
#  1. Joas Tsiri
#  2. Casper
#  3. Nthabiseng
#  4. Rizquah
#  5. Tshiamo

# Due Date: 03/02/2021
# 
# Â© Explore Data Science Academy

# <div><b> <h1>Table of contents</h1> </b>
#     <ol><li> <a>  Introduction</a> </li>
#         <li><a>Loading Datasets</a></li>
#     <li><a>Merge the datasets<a/></li>
#         <li><a></a></li>
#     <li><a></a></li>
#         <li><a></a></li>
#         <li><a></a></li>
#     
# </ol>

# ## 1. Introduction
# 
# The expansion of information on the internet provides a wide range of options while also increasing the complexity of what the user is looking for. To compensate for this complexity, the recommendation system employs machine learning to analyze the user's past preferences.
# A recommender system can be used in a variety of contexts, including marketing and consumer websites. When one uses the Takealot website to order a product, the site will recommend other products based on what others have purchased. A recommender system is essentially a system that takes the user's selection as input and predicts all of the related movies, news, books, and so on. 
# 
# There are primarily two types of recommender systems, with a third possibly combining the first two. The first is collaborative filtering and the second is content filtering. Collaborative filtering builds a model based on the previous behavior of a user (past purchases) and similar users. Personalized recommendations are often generated by collaborative filtering. Netflix, iTunes, YouTube, Amazon, and IMDB all use collaborative filtering. ##Discuss the problem statement here##.
# 
# There are two parts to this project. In the first part, we will learn about EDA and recommender systems, and we will create a Movie Recommender System (Content-based Filtering). Secondly, we will deploy the model to the streamlit  website and get live predictions.

# #### Importing libraries

# In[4]:


# Data manipulation
import numpy as np 
import pandas as pd   
import re

# Data visualization
import matplotlib
import seaborn as sns 
import matplotlib.pyplot as plt 
from wordcloud import WordCloud, STOPWORDS 

# Style
import matplotlib.style as style 
sns.set(font_scale=1.5)
style.use('seaborn-pastel')
sns.set(style="whitegrid")
sns.set_style("dark")

#to merge dataframes
from functools import reduce

#function to view dataframes side by side
from IPython.display import display_html
from itertools import chain,cycle
def display_side_by_side(*args,titles=cycle([''])):
    html_str=''
    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):
        html_str+='<th style="text-align:center"><td style="vertical-align:top">'
        html_str+=f'<h2>{title}</h2>'
        html_str+=df.to_html().replace('table','table style="display:inline"')
        html_str+='</td></th>'
    display_html(html_str,raw=True)

# Ignore warnings 
import warnings
from pandas.core.common import SettingWithCopyWarning
warnings.simplefilter(action="ignore", category=SettingWithCopyWarning)
    
# Building recommender systems
from surprise import Dataset
from surprise import Reader
from surprise import accuracy
from surprise import SVD
from surprise import KNNWithMeans
from surprise.model_selection import cross_validate
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
#libraries to preprocess data
import nltk
import itertools
from nltk.corpus import stopwords
import string
from tqdm.auto import tqdm
from sklearn.feature_extraction.text import CountVectorizer   
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer 
from sklearn.model_selection import train_test_split
# Building classification models
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
# Model evaluation
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score


# #### Loading and visualizing datasets

# In[3]:


#loading data 
train = pd.read_csv('train.csv')
tags = pd.read_csv('tags.csv')
movies= pd.read_csv('movies.csv')
imbd_data = pd.read_csv('imdb_data.csv')
genome_tags = pd.read_csv('genome_tags.csv')
genome_scores =pd.read_csv('genome_scores.csv')
#imbd_data =pd.read_csv('imdb_data.csv')
links= pd.read_csv('links.csv')

# visualize the dataset
display_side_by_side(train.head(2),movies.head(2),imbd_data.head(1), tags.head(2),genome_tags.head(2),
                     genome_scores.head(2),links.head(2),titles =
                   ['train','movies','imbd_data', 'tags', 'genome_tags', 'genome_scores', 'links'])


# In[4]:


#shape of the datasets
print(f"genome_tags shape is {genome_tags.shape}")
print(f"genome_scores shape is {genome_scores.shape}")
print(f"movies shape is {movies.shape}")
print(f"imbd_data shape is {imbd_data.shape}")
print(f"links shape is {links.shape}")
print (f'train shape is{train.shape}')
print (f'tags shape is{tags.shape}')


# In[5]:


#check null values of the datasets
print(f"genome_tags null values is: {genome_tags.isnull().mean()*100}")
print(f"genome_scores null values is: {genome_scores.isnull().mean()*100}")
print(f"movies shape null values is: {movies.isnull().mean()*100}")
print(f"imbd_data null values is: {imbd_data.isnull().mean()*100}")
print(f"links null values is: {links.isnull().mean()*100}")
print (f'train null values is:{train.isnull().mean()*100}')
print (f'tags null values is:{tags.isnull().mean()*100}')


# <b> observations </b>
# The datasets are large, with over 10 million records in the train, tags, and genome tags. This may cause processing issues because it takes time to run on the personal computer. It can also be seen that imbd dataframe has highly significant missing values on all features with a percentage greater than 30%. In addition, the tag dataFrame feature 'tag' has insignificant missing values.

# <b> Merging datasets </b>

# In[14]:


df = imbd_data[['movieId','title_cast','director', 'plot_keywords']]
df = df.merge(movies[['movieId', 'genres', 'title']], on='movieId', how='inner')
df.head()


# In[15]:


# Convert data types to strings for string handling
df['title_cast'] = df.title_cast.astype(str)
df['plot_keywords'] = df.plot_keywords.astype(str)
df['genres'] = df.genres.astype(str)
df['director'] = df.director.astype(str)

# Removing spaces between names
df['director'] = df['director'].apply(lambda x: "".join(x.lower() for x in x.split()))
df['title_cast'] = df['title_cast'].apply(lambda x: "".join(x.lower() for x in x.split()))

# Discarding the pipes between the actors' full names and getting only the first three names
df['title_cast'] = df['title_cast'].map(lambda x: x.split('|')[:3])

# Discarding the pipes between the plot keywords' and getting only the first five words
df['plot_keywords'] = df['plot_keywords'].map(lambda x: x.split('|')[:5])
df['plot_keywords'] = df['plot_keywords'].apply(lambda x: " ".join(x))

# Discarding the pipes between the genres 
df['genres'] = df['genres'].map(lambda x: x.lower().split('|'))
df['genres'] = df['genres'].apply(lambda x: " ".join(x))
df.head()


# In[19]:


#In movie.csv from the Title column we can extrct the year in which the movie was released.

def year(title):
    year=re.search(r'\(\d{4}\)', title)
    if year:
       year=year.group()
       return int(year[1:5])
    else:
        return 0


# In[21]:


#Genres are a pipe-separated list
df['movie_year'] = df['title'].apply(year)
df.head()


# The Timestamp column from both tags.csv and ratings.csv does not contribute much. So lets drop them
# 
# 
# 

# In[ ]:





# In[17]:


#merge two tables on movieId
movie_df = pd.merge(train,movies, on  = 'movieId')
movie_df.drop('timestamp', axis=1, inplace=True)
movie_df


# #### Explanatory Data Analysis

# In[ ]:


We wills tart by a creating a dataframe that we can visualize the ratings


# In[18]:


#dataframe for average rating
avg_movie_df = movie_df.groupby('title')['rating'].mean().sort_values(ascending = False).reset_index().rename(columns= {'rating': 'average_rating'})
# dataframe count rating
movies_ranting_count = movie_df.groupby('title')['rating'].count().sort_values(ascending = True).reset_index().rename(columns= {'rating': 'rating_count'})
#meging the two dataframes
movies_rating_count_df = movies_ranting_count.merge(round(avg_movie_df,0), on= 'title')
movies_rating_count_df


# In[14]:


plt.figure(figsize =(10,5))
plt.subplot(1,2, 1)
sns.histplot(data= movies_rating_count_df, x= 'average_rating', bins = 10)

plt.subplot(1,2, 2)
sns.histplot(data= movie_df, x= 'rating', bins = 10, color= 'orange')


# Observation: The plots show that the rating distribution is different. When the rating is taken without the average, there are a large number of rated movies, which can lead to the bias of thinking a movie is highly rated when it was only rated by a few people.

# In[19]:


def most_watched(input_df):  
    """"
    This function creates a plot showing the popularity of each genre 
    over the last 50 years.
    
    Input: input_df
           datatype: DataFrame
           
    Output: None
    
    """   
    # Create a copy of the input dataframe and merge it to the movies dataframe
    df = input_df.copy()
    df = df.merge(movies,on='movieId',how='left')
    
    # Create an empty dataframe
    b = pd.DataFrame()
    
    # Extract the timestamps and genres 
    timestamps = [timestamp for timestamp in df.timestamp]
    #all_genres = set(','.join([genres.replace('|',',') for genres in df.genres]).split(','))
    
    # Get the number of ratings for each genre for each year since 1970
    for index,genre in enumerate(all_genres):
        a = pd.Series([int((timestamps[i]/31536000)+1970) for i,x in enumerate(df.genres) if genre in x])
        a = a.value_counts()
        b = pd.concat([b,pd.DataFrame({genre:a})],axis=1)
    
    # Plot the trends for each genre on the same line graph 
    plt.figure(figsize=(20,10))
    plot = sns.lineplot(data=b, dashes=False)
      
    # Add plot labels
    plt.title('Trends in genre popularity',fontsize=20)
    plt.xlabel('Years', fontsize=15)
    plt.ylabel('Number of ratings', fontsize=15)
    
    plt.show()
    
    return

most_watched(train)


# 

# In[ ]:





# In[30]:


all_data_movie_df = movie_df.merge(movies_ranting_count, left_on = 'title',right_on = 'title', how = 'left')
all_data_movie_df


# In[20]:


def wordcloud_generator(df, column):  
    """
    This function extracts all the unique keywords in a column
    and counts the number of times each keyword occurs in the column
    while ignoring words that are not meaningful.
    these keywords are then used to generate a word cloud 
    
    Input: df
           datatype: DataFrame
           column
           datatype: str
           
    Output: wordcloud
            Datatype: None
            
    """    
    keyword_counts = {}
    keyword_pair = []
    words = dict()
    
    # list of words that should be ignored
    ignore = ['nan', ' nan', 'nan ', 'seefullsummary', ' seefullsummary', 'seefullsummary ']
    
    # Extract the unique keywords 
    for keyword in [keyword for keyword in df[column] if keyword not in ignore]:
        if keyword in keyword_counts.keys():
            keyword_counts[keyword] += 1
        else:
            keyword_counts[keyword] = 1
     
    # Pair the keywords with their frequencies
    for word,word_freq in keyword_counts.items():
        keyword_pair.append((word,word_freq))
       
    # Sort the keywords accprding to their frequencies
    keyword_pair.sort(key = lambda x: x[1],reverse=True)
    
    # Make it wordcloud-ready
    for s in keyword_pair:
        words[s[0]] = s[1]
        
    # Create a wordcloud using the top 2000 keywords
    wordcloud = WordCloud(width=800, 
                          height=500, 
                          background_color='black', 
                          max_words=2000,
                          max_font_size=110,
                          scale=3,
                          random_state=0,
                          colormap='Greens').generate_from_frequencies(words)

    return wordcloud 


# In[21]:


# Plot wordcloud
plot_keywords = wordcloud_generator(df, 'plot_keywords')
f = plt.figure(figsize=(20, 8)) 
plt.imshow(plot_keywords) 
plt.axis('off') 
plt.title('Plot keywords\n', fontsize=25)
plt.show()


# In[22]:


# Plot genres
plot_genres = wordcloud_generator(df, 'genres')
f = plt.figure(figsize=(20, 8)) 
plt.imshow(plot_keywords) 
plt.axis('off') 
plt.title('Plot keywords\n', fontsize=25)
plt.show()


# In[ ]:


plot_genres = wordcloud_generator(df, 'genres')
f = plt.figure(figsize=(20, 8)) 
plt.imshow(plot_keywords) 
plt.axis('off') 
plt.title('Plot keywords\n', fontsize=25)
plt.show()

fig = plt.figure(1, figsize=(18,13))
ax2 = fig.add_subplot(2,1,2)
y_axis = [i[1] for i in trunc_occurences]
x_axis = [k for k,i in enumerate(trunc_occurences)]
x_label = [i[0] for i in trunc_occurences]
plt.xticks(rotation=85, fontsize = 15)
plt.yticks(fontsize = 15)
plt.xticks(x_axis, x_label)
plt.ylabel("No. of occurences", fontsize = 24, labelpad = 0)
ax2.bar(x_axis, y_axis, align = 'center', color='r')
plt.title("Popularity of Genres",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 30)
plt.show()


# In[23]:


# Generate a wordcloud using the 2000 most frequently occuring actors' names in the dataset
df['title_cast'] = df['title_cast'].apply(lambda x: " ".join(x))
actors = wordcloud_generator(df, 'title_cast')

# plot the WordCloud                        
f = plt.figure(figsize=(20, 8)) 
plt.imshow(actors) 
plt.axis('off') 
plt.title('Actors\n', fontsize=25)
plt.show()


# In[ ]:





# In[44]:






#sample(10000)
#plt.figure(figsize =(10,9))
#sns.jointplot(x= 'rating', y = 'rating_count', data =all_data_movie_df , palette='YlGnBu')


# In[31]:


pd.set_option('display.float_format', lambda x: '%.3f' % x)
print (all_data_movie_df['rating_count'].describe())


# In[32]:


threshold = 50
top_rated_movies = all_data_movie_df[all_data_movie_df['rating_count']>= threshold]
top_rated_movies


# In[28]:


#top_rated_movies =top_rated_movies['timestamp'].drop, axis =1, inplace = True)
#top_rated_movies.head


# <b>Getting a base model submission using the train dataframe</b>

# In[25]:


# Creating an empty column and list to store the corpus for each movie
df['abc'] = ''
abc = []

# List of the columns we want to use to create our corpus 
columns = ['title_cast', 'director', 'plot_keywords', 'genres']

# For each movie, combine the contents of the selected columns to form it's unique corpus 
for i in range(0, len(df['movieId'])):
    words = ''
    for col in columns:
        words = words + df.iloc[i][col] + " "        
    abc.append(words)

# Add the corpus information for each movie to the dataframe 
df['abc'] = abc
df.set_index('movieId', inplace=True)

# Drop the columns we don't need anymore to preserve memory
df.drop(columns=['title_cast', 'director', 'plot_keywords', 'genres'], inplace=True)

df.head()


# In[ ]:





# In[41]:


cv = CountVectorizer()
count_matrix = cv.fit_transform(df['abc'])


# In[42]:


cos_sim = cosine_similarity(count_matrix, count_matrix)
print(cos_sim.shape)
cos_sim


# In[ ]:


# Creating a small test dataframe to evaluate our models
tests = train.copy()
tests.drop(['timestamp'], axis=1, inplace=True)
tests = tests.head(10000)

# Creating the training data
reader = Reader(rating_scale=(0.5, 5))
test_data = Dataset.load_from_df(tests[['userId','movieId','rating']], reader)

# Compute similarities between users using cosine distance
sim_options = {"name": "cosine",
               "user_based": True}  

# Evaluate the model 
user = KNNWithMeans(sim_options=sim_options)
cv = cross_validate(user, test_data, cv=5, measures=['RMSE'], verbose=True)


# In[ ]:


SVM

from sklearn.svm import SVC 
svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train) 
svm_predictions = svm_model_linear.predict(X_test) 
  
# model accuracy for X_test   
accuracy = svm_model_linear.score(X_test, y_test) 
  
# creating a confusion matrix 
cm = confusion_matrix(y_test, svm_predictions) 
accuracy
#cm


# In[ ]:


# Evaluate the model 
svd = SVD(random_state=0)
cv = cross_validate(svd, test_data, cv=5, measures=['RMSE'], verbose=True)


# In[ ]:


#KNN

from sklearn.neighbors import KNeighborsClassifier 
knn = KNeighborsClassifier(n_neighbors = 7).fit(X_train, y_train) 
  
# accuracy on X_test 
accuracy = knn.score(X_test, y_test) 
  
# creating a confusion matrix 
knn_predictions = knn.predict(X_test)  
cm = confusion_matrix(y_test, knn_predictions) 

accurac


# In[34]:


top_rated_movies = top_rated_movies.iloc[:9744920,:]
# lets make a pivot table in order to make rows are users and columns are movies. And values are rating
pivot_table = top_rated_movies.pivot_table(index = ["title"],columns = ["userId"],values = "rating").fillna(0)
pivot_table


# In[ ]:





# In[ ]:





# In[33]:


movie_watched = pivot_table["Vertigo (1958)"]
similarity_with_other_movies = pivot_table.corrwith(movie_watched)  # find correlation between "Bad Boys (1995)" and other movies
similarity_with_other_movies = similarity_with_other_movies.sort_values(ascending=False)
similarity_with_other_movies.head()


# we can pass the pivot table to the KNN model

# In[39]:


#using scikit learn for PCA

#scale the data
from sklearn.preprocessing import  StandardScaler
#scale our data
scaler = StandardScaler()
scaled_x =scaler.fit_transform(pivot_table)
scaled_x


# Everything is now shifted and the mean is now close to zero

# In[41]:


#doing PCA to two priciple components
from sklearn.decomposition import PCA
pca_model = PCA(n_components= 30)


# In[44]:


#checking 
pca_total = PCA(n_components = 80)
pca_total.fit(scaled_x)


# In[46]:


np.sum(pca_total.explained_variance_ratio_)


# In[48]:


explained_variance = []
for n in range(1,80):
    pca=PCA(n_components = n)
    pca.fit(scaled_x)
    
    explained_variance.append(np.sum(pca.explained_variance_ratio_))


# In[49]:


plt.plot(range(1,80), explained_variance)
plt.xlabel('num of components')
plt.ylabel('Varience_explained')


# In[51]:


#pca model
pca_model = PCA(n_components= 30)
#fit to calculate the eigenvalues
pca_model.fit(scaled_x)
pca_results = pca_model.transform(scaled_x)


# In[53]:


plt.scatter(pca_results[:,0], pca_results[:,1])


# In[54]:


#max varience in the data
pca_model.components_


# In[55]:


#explained varience ratio
np.sum(pca_model.explained_variance_ratio_)


# In[56]:


from scipy.sparse import csr_matrix
pivot_table_df_matrix = csr_matrix(pivot_table.values)


# In[57]:


from sklearn.neighbors import NearestNeighbors
model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(pivot_table_df_matrix)


# In[58]:


pivot_table_df_matrix.shape


# In[59]:


query_index = np.random.choice(pivot_table.shape[0])
print(query_index)
distances, indices = model_knn.kneighbors(pivot_table.iloc[query_index,:].values.reshape(1, -1), n_neighbors = 6)
query_index=196


# In[61]:


pivot_table.head()


# In[62]:


for i in range(0, len(distances.flatten())):
    if i == 0:
        print('Recommendations for {0}:\n'.format(movie_features_df.index[query_index]))
    else:
        print('{0}: {1}, with distance of {2}:'.format(i, movie_features_df.index[indices.flatten()[i]], distances.flatten()[i]))


# In[ ]:




